{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# QA over SQL data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a73fbf-e241-499d-a235-32a87b46ee7c",
   "metadata": {},
   "source": [
    "## Intro\n",
    "* We will create a Q&A app over tabular data in databases.\n",
    "* These app will allow us to **ask a question about the data in a database in natural language and get back an answer also in natural language**.\n",
    "* Building Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. **In production, make sure that your database connection permissions** are always scoped as narrowly as possible for your chain's needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a508d2ac-5e64-48a6-895e-2e0ab5162d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "## Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411cb8c-4baf-43cf-bf79-e56646c90519",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b4581b-0b47-4a00-8c72-5d4846cfea15",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8595e-5c07-4b02-8a79-db55fd357527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373482d-492a-40df-b0ac-131ed70c5259",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc942a2e-b9d3-4f5f-a612-f212cacc8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a99a77-abc6-412a-b9dc-651e00fe5867",
   "metadata": {},
   "source": [
    "## Connect with the database\n",
    "* In this example we will use a SQLite connection with the `street_tree_db` database we have in the `data` folder.\n",
    "* **Remember to download the data folder from the Github repository**.\n",
    "* The `data` folder must be in the root directory of this app.\n",
    "* We can interface with the database using the SQLAlchemy-driven `SQLDatabase` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ca291-4c3f-47a6-8854-42d2ac953c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "sqlite_db_path = \"data/street_tree_db.sqlite\"\n",
    "\n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{sqlite_db_path}\")\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a7a0a-059c-496f-91ee-1303aa1bb6ae",
   "metadata": {},
   "source": [
    "We can create a simple chain that takes a question and does the following:\n",
    "* Convert the question into a SQL query;\n",
    "* Execute the query;\n",
    "* Use the result to answer the original question.\n",
    "\n",
    "The first step in a SQL chain is to take the user input and convert it to a SQL query. LangChain comes with a **built-in chain** for this, `create_sql_query_chain`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9f155-cacc-41ad-bce8-1d9ceb318329",
   "metadata": {},
   "source": [
    "#### Step 1: Translating a question in natural language into an SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b7787-2076-4d6a-94ad-8c815fb1ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_sql_query_chain\n",
    "\n",
    "chain = create_sql_query_chain(llm, db)\n",
    "\n",
    "response = chain.invoke({\"question\": \"How many species of trees are in San Francisco?\"})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c5f7fa-0147-423f-8793-22fdbdacdd29",
   "metadata": {},
   "source": [
    "* We can execute the query to make sure it's valid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b598141-a7c3-4209-b0cf-626b28d677b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.run(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71188e6d-8b78-4162-804a-918d54449b17",
   "metadata": {},
   "source": [
    "* We can also inspect the chain directly for its prompts. Pay attention to the second paragraph, where it says: **\"Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\"** This is a limitation and if it is not handled well can cause errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fe095e-a138-4d5e-a02a-14c6d38d1786",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.get_prompts()[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eab168-ba8d-410f-a158-631d87edb2d9",
   "metadata": {},
   "source": [
    "#### Step 2: Executing the SQL query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef39e5-1b37-4673-aa4c-951fbb3e6e3f",
   "metadata": {},
   "source": [
    "* Now that we've generated a SQL query, we'll want to execute it.\n",
    "* When you are in production, consider carefully if it is OK to run automated queries over your data. Minimize the database connection permissions as much as possible. Consider adding a human approval step to you chains before query execution.\n",
    "* We can **use the QuerySQLDatabaseTool to easily add query execution to our chain**:\n",
    "    * The user asks a question.\n",
    "    * The write_query chain has que question as input and the SQL query as output.\n",
    "    * The execute_query chain has the SQL query as input and the SQL query execution as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e849055-8d6b-49ed-b8b3-aeb95e04b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "\n",
    "write_query = create_sql_query_chain(llm, db)\n",
    "\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "\n",
    "chain = write_query | execute_query\n",
    "\n",
    "chain.invoke({\"question\": \"List the species of trees that are present in San Francisco\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998cea6-43ff-4376-9a83-618995ee36a6",
   "metadata": {},
   "source": [
    "#### Step 3: Translate the SQL response into a natural language response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52988b3-93fd-4190-b6a5-0755fe28ecf9",
   "metadata": {},
   "source": [
    "* Now that we've got a way to generate and execute queries, we need to **combine the original question and SQL query result with the chat model to generate a final answer in natural language**.\n",
    "* We can do this by passing question and result to the LLM like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24dee97-465f-4979-9afe-79e0b7eb6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "sqlite_db_path = \"data/street_tree_db.sqlite\"\n",
    "\n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{sqlite_db_path}\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, \n",
    "    corresponding SQL query, and SQL result, \n",
    "    answer the user question.\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Result: {result}\n",
    "Answer: \"\"\"\n",
    ")\n",
    "\n",
    "write_query = create_sql_query_chain(llm, db)\n",
    "\n",
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "\n",
    "#chain = write_query | execute_query\n",
    "\n",
    "#chain.invoke({\"question\": \"List the species of trees that are present in San Francisco\"})\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(query=write_query).assign(\n",
    "        result=itemgetter(\"query\") | execute_query\n",
    "    )\n",
    "    | answer_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"List the species of trees that are present in San Francisco\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc31c07-a003-403d-bea6-c8965f9e8b38",
   "metadata": {},
   "source": [
    "#### Let's review what is happening in the above chain.\n",
    "* The user asks a question (identified by the variable name \"question\").\n",
    "* We use RunnablePassthrough to get that \"question\" variable, and we use .assign() twice to get the other two variables required by the prompt template: \"query\" and \"result\".\n",
    "* With the first .assign(), the write_query chain has que question as input and the SQL query (identified by the variable name \"query\") as output.\n",
    "* With the second .assign(), the execute_query chain has the SQL query as input and the SQL query execution (identified by the variable name \"result\") as output.\n",
    "* The prompt template has the question (identified by the variable name \"question\"), the SQL query (identified by the variable name \"query\") and the SQL query execution (identified by the variable name \"result\") as input, and the final prompt as the output.\n",
    "* The chat model has the prompt as he input and the AIMessage with the response in natural language as the output.\n",
    "* The StrOutputParser has the AIMessage with the response in natural language as the input and the response in natural language as a string of text as the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcad81-c02c-4f66-9a51-a5ceb15c6f5d",
   "metadata": {},
   "source": [
    "#### About the role of .assign() in this chain\n",
    "In this exercise we have learned more about the .assign() built-in method for Runnables. We have seen that the .assign() method **allows us to include additional keys and values** based on existing input.\n",
    "* With the first .assign(), the write_query chain has the question as input and the SQL query (identified by the variable name \"query\") as output.\n",
    "* With the second .assign(), the execute_query chain has the SQL query as input and the SQL query execution (identified by the variable name \"result\") as output.\n",
    "\n",
    "The .assign() method allows us to add new keys and values to the output of a Runnable **while keeping the original keys and values intact**. See the process again:\n",
    "* We use RunnablePassthrough to get that \"question\" variable, and we use .assign() twice to get the other two variables required by the prompt template: \"query\" and \"result\".\n",
    "* With the first .assign(), the write_query chain has que question as input and the SQL query (identified by the variable name \"query\") as output.\n",
    "* With the second .assign(), the execute_query chain has the SQL query as input and the SQL query execution (identified by the variable name \"result\") as output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa144596-cd5d-498c-97e0-4406f1e9f69f",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 004-invoke-stream-batch.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 001-qa-from-sql.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab30035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
