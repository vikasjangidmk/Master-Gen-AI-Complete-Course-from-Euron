{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# QA over PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a73fbf-e241-499d-a235-32a87b46ee7c",
   "metadata": {},
   "source": [
    "## Intro\n",
    "* We will create a Q&A app that can answer questions about PDF files.\n",
    "* We will use a Document Loader to load text in a format usable by an LLM, then build a retrieval-augmented generation (RAG) pipeline to answer questions, including citations from the source material.\n",
    "* **We will use a basic approach for this project. You will see more advanced ways to solve the same problem in next projects**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "597f1e7d-75bd-4453-b4a4-7aecdb9413e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "## Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1b8d6-7d28-4b7c-88eb-15af168a51aa",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4732291c-e0a0-496b-b45d-5cfd0424d04a",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c8d482",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae8595e-5c07-4b02-8a79-db55fd357527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99edb898-4503-429d-ad7d-fad4fc74300f",
   "metadata": {},
   "source": [
    "## Load the PDF file\n",
    "* The loader reads the PDF at the specified path into memory.\n",
    "* It then extracts text data using the pypdf package.\n",
    "* Finally, it creates a LangChain Document for each page of the PDF with the page's content and some metadata about where in the document the text came from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1e2ab-2870-43a0-bfdf-4da12047c1d1",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following packages because they are already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6903027c-722c-4b9f-8717-b3affb844227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a191a2-133a-4bae-be2c-0b9f226c0e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "764957d8-b4b3-4555-affc-187203e85293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./data/Be_Good.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cb64f5c-d5da-4c60-9418-a9d5407faa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be Good - Essay by Paul Graham\n",
      "Be Good\n",
      "Be good\n",
      "April 2008(This essay is derived from a talk at the 2\n",
      "{'source': './data/Be_Good.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[0:100])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1148577-8c06-47fb-9d48-fa9ce7f14e30",
   "metadata": {},
   "source": [
    "## RAG\n",
    "* We will use the vector database (aka. vector store) Chroma DB.\n",
    "* Using a text splitter, we will split the loaded PDF into smaller documents that can more easily fit into an LLM's context window, then load them into a vector store.\n",
    "* We can then create a retriever from the vector store for use in our RAG chain:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cfdcb7-6f73-46a9-9a28-6c1550021088",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21711cdc-510e-4738-b8ba-28135b4b7539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ae6680-d314-435f-9213-ffb66bac7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f674aa-167a-46f1-8e5c-c7f62d237092",
   "metadata": {},
   "source": [
    "#### We will use two pre-defined chains to construct the final rag_chain:\n",
    "In this exercise we are going to use two pre-defined chains to build the final chain:\n",
    "* create_stuff_documents_chain\n",
    "* create_retrieval_chain\n",
    "* Let's learn a little bit more about these two pre-defined chains.\n",
    "\n",
    "#### create_stuff_documents_chain\n",
    "The create_stuff_documents_chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window of the LLM you are using.\n",
    "1. **Taking a List of Documents**: This function starts by receiving a group of documents that you provide.\n",
    "  \n",
    "2. **Formatting into a Prompt**: It then takes all these documents and organizes them into a specific prompt. A prompt is essentially a text setup that is used to feed information into a language model (like an LLM, or Large Language Model).\n",
    "\n",
    "3. **Passing to an LLM**: After formatting the documents into a prompt, this function sends the formatted prompt to a language model. The model will process this information to perform tasks like answering questions, generating text, etc.\n",
    "\n",
    "4. **Fit within Context Window**: The function sends all the documents at once to the LLM. However, it's important to make sure that the total length of the prompt does not exceed what the LLM can handle at one time. This limit is known as the \"context window\" of the LLM. If the prompt is too long, the model might not process it effectively.\n",
    "\n",
    "In simpler terms, think of this chain as a way of taking several pieces of text, bundling them together in a specific way, and then feeding them to an LLM that reads and uses this bundled text to do its job. Just make sure the bundle isnâ€™t too big for the LLM to handle at once!\n",
    "\n",
    "\n",
    "#### create_retrieval_chain\n",
    "The create_retrieval_chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a response.\n",
    "1. **Receiving a User Inquiry**: This process begins when a user asks a question or makes a request.\n",
    "\n",
    "2. **Using a Retriever to Fetch Documents**: The function then uses a retriever to find documents that are relevant to the user's inquiry. This means it searches through available information to pick out parts that can help answer the question.\n",
    "\n",
    "3. **Passing Information to an LLM**: After gathering the relevant documents, both these documents and the original user inquiry are sent to an LLM.\n",
    "\n",
    "4. **Generating a Response**: The LLM processes all the information it receives to come up with an appropriate response, which is then given back to the user.\n",
    "\n",
    "In simpler terms, this chain acts like a smart assistant that first looks up information based on your question, gathers useful details, and then uses those details along with your original question to craft a helpful answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac58036b-b6bc-44d6-8d76-b6141d8a9da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article discusses the importance of creating something that people want, advising founders not to worry excessively about the business model initially. It also touches on the concept of approaching startups like charities, emphasizing the value of focusing on user satisfaction and the benefits of launching quickly to gather valuable feedback.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What is this article about?\"})\n",
    "\n",
    "results[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89746b08-1ae3-4f9b-971d-449a32d9c7e7",
   "metadata": {},
   "source": [
    "* If you print the whole `results` you will see that **you get both the answer, and the context the LLM used to generate that answer**. See it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87e341ec-36f7-4703-97d5-ce71cd2ecac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is this article about?',\n",
       " 'context': [Document(metadata={'page': 0, 'source': './data/Be_Good.pdf'}, page_content=\"Be Good - Essay by Paul Graham\\nBe Good\\nBe good\\nApril 2008(This essay is derived from a talk at the 2008 Startup School.)About a month after we\\nstarted Y Combinator we came up with the\\nphrase that became our motto: Make something people want.  We've\\nlearned a lot since then, but if I were choosing now that's still\\nthe one I'd pick.Another thing we tell founders is not to worry too much about the\\nbusiness model, at least at first.  Not because making money is\\nunimportant, but because it's so much easier than building something\\ngreat.A couple weeks ago I realized that if you put those two ideas\\ntogether, you get something surprising.  Make something people want.\\nDon't worry too much about making money.  What you've got is a\\ndescription of a charity.When you get an unexpected result like this, it could either be a\\nbug or a new discovery.  Either businesses aren't supposed to be\\nlike charities, and we've proven by reductio ad absurdum that one\"),\n",
       "  Document(metadata={'page': 10, 'source': './data/Be_Good.pdf'}, page_content=\"Be Good - Essay by Paul Graham\\nGoogle does.Most explicitly benevolent projects don't hold themselves sufficiently\\naccountable.  They act as if having good intentions were enough to\\nguarantee good effects.[3] Users dislike their\\nnew operating system so much that they're starting petitions to\\nsave the old one.  And the old one was nothing special.  The hackers\\nwithin Microsoft must know in their hearts that if the company\\nreally cared about users they'd just advise them to switch to OSX.Thanks to Trevor Blackwell, Paul\\nBuchheit, Jessica Livingston,\\nand Robert Morris for reading drafts of this.\\nPage 11\"),\n",
       "  Document(metadata={'page': 1, 'source': './data/Be_Good.pdf'}, page_content=\"Be Good - Essay by Paul Graham\\ncaptains always try to get upwind\\nof their opponents.  If you're upwind, you decide when and if to\\nengage the other ship.  Craigslist is effectively upwind of enormous\\nrevenues.  They'd face some challenges if they wanted to make more,\\nbut not the sort you face when you're tacking upwind, trying to\\nforce a crappy product on ambivalent users by spending ten times\\nas much on sales as on development.  [1]I'm not saying startups should aim to end up like\\nCraigslist.\\nThey're a product of unusual circumstances.  But they're a good\\nmodel for the early phases.Google looked a lot like a charity in the beginning. They didn't\\nhave ads for over a year.  At year 1, Google was indistinguishable\\nfrom a nonprofit.  If a nonprofit or government organization had\\nstarted a project to index the web, Google at year 1 is the limit\\nof what they'd have produced.Back when I was working on spam filters I thought it would be a\"),\n",
       "  Document(metadata={'page': 5, 'source': './data/Be_Good.pdf'}, page_content=\"Be Good - Essay by Paul Graham\\nyou've basically built yourself a giant tamagotchi.  You've made\\nsomething you need to take care of.Blogger is a famous example of a startup that went through\\nreally\\nlow lows and survived.  At one point they ran out of money and\\neveryone left. Evan Williams came in to work the next day, and there\\nwas no one but him.  What kept him going?  Partly that users needed\\nhim.  He was hosting thousands of people's blogs. He couldn't just\\nlet the site die.There are many advantages of launching quickly, but the most important\\nmay be that once you have users, the tamagotchi effect kicks in.\\nOnce you have users to take care of, you're forced to figure out\\nwhat will make them happy, and that's actually very valuable\\ninformation.The added confidence that comes from trying to help people can\\nalso help you with investors. One of the founders of \\nChatterous told \\nme recently that he and his cofounder had decided that this service\")],\n",
       " 'answer': 'The article discusses the importance of creating something that people want, advising founders not to worry excessively about the business model initially. It also touches on the concept of approaching startups like charities, emphasizing the value of focusing on user satisfaction and the benefits of launching quickly to gather valuable feedback.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7225ee-ff03-4c41-a4b5-1138ab3cfbfb",
   "metadata": {},
   "source": [
    "* Examining the values under the context further, you can see that they are documents that each contain a chunk of the ingested page content. These documents also preserve the original **metadata** from way back when you first loaded them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2153a091-c9cb-489e-b505-c9691b4cc774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 0, 'source': './data/Be_Good.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(results[\"context\"][0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a05cb-2700-4f40-9b39-5313f636391e",
   "metadata": {},
   "source": [
    "* This particular chunk came from page 0 in the original PDF. You can use this data to show which page in the PDF the answer came from, allowing users to quickly verify that answers are based on the source material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9342167-0f8a-44e6-a2cb-53211bb34c45",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 004-invoke-stream-batch.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 001-qa-from-pdf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1e1fd97-edd3-4142-9e1a-adefb0fe5c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The End\n"
     ]
    }
   ],
   "source": [
    "print(\"The End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
