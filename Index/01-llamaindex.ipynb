{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4b8964-2ceb-4aa7-aa50-8f2934c017c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8faf7ee-579e-45e2-b22c-4c5ebf7e2d98",
   "metadata": {},
   "source": [
    "## Load private document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9cd4d-173b-4f4a-a992-8a770c741d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0238d1-3bcf-4ded-a769-98d8d93729ef",
   "metadata": {},
   "source": [
    "## Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63975112-0f22-4edd-8c7b-1b4406f8e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a159feeb-7eb4-4911-bbf1-714de67dfe69",
   "metadata": {},
   "source": [
    "## Ask questions to private document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede51e4-1aec-4a70-9007-7a15b4e6a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"summarize the article in 100 words\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c19db6-15c5-497a-b018-9c3a3cf12511",
   "metadata": {},
   "source": [
    "## See under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215fa5e0-ee57-48fa-a343-c9662065709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import sys\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     stream=sys.stdout, \n",
    "#     level=logging.DEBUG\n",
    "# )\n",
    "\n",
    "# logging.getLogger().addHandler(\n",
    "#     logging.StreamHandler(stream=sys.stdout)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c62d2e-4ebc-473a-b215-8d719e87fe3e",
   "metadata": {},
   "source": [
    "## Save the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0043ba-f669-4c0f-b67f-1415da83c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "# check if storage already exists\n",
    "if not os.path.exists(\"./storage\"):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist()\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ac6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"According to the author, what is good?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c257e6-0fac-4a32-b1ac-b78338d3a929",
   "metadata": {},
   "source": [
    "## Customization options\n",
    "* parse into smaller chunks\n",
    "* use a different vector store\n",
    "* retrieve more context when I query\n",
    "* use a different LLM\n",
    "* use a different response mode\n",
    "* stream the response back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f150c-7df3-48f9-98fa-b8bd55f696c8",
   "metadata": {},
   "source": [
    "## Use cases\n",
    "* QA\n",
    "* Chatbot\n",
    "* Agent\n",
    "* Structured Data Extraction\n",
    "* Multimodal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf6e50-31b8-45fe-af63-311876d79641",
   "metadata": {},
   "source": [
    "## Optimizing\n",
    "* Advanced Retrieval Strategies\n",
    "* Evaluation\n",
    "* Building performant RAG applications for production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a8a462-4952-48cc-84cb-864841cecbc8",
   "metadata": {},
   "source": [
    "## Other\n",
    "* LlamaPacks and Create-llama = LangChain templates\n",
    "* Very recent, still in beta\n",
    "* Very interesting: create-llama allows you to create a Vercel app!\n",
    "* Very interesting: open-source end-to-end project (SEC Insights)\n",
    "    * llamaindex + react/nextjs (vercel) + fastAPI + render + AWS\n",
    "    * environment setup: localStack + docker\n",
    "    * monitoring: sentry\n",
    "    * load testing: loader.io\n",
    "    * [web](https://www.secinsights.ai/)\n",
    "    * [code](https://github.com/run-llama/sec-insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59ffe0-ded8-41ef-9c1b-3d4af53215bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
